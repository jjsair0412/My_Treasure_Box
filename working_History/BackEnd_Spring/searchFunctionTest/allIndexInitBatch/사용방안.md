# Batch Project 사용방안
## 사전 사항 - elasticsearch 구축
elastic Search를 아래 문서를 따라 구축합니다.

### ***주의***
elastic search API들은 elastic search에 접근할 때 , 토큰기반 인증 또는 ID & PWD 기반 인증을 해야 합니다.

**따라서 elasticsearch를 구축할 때 , 유저를 같이 생성해줘야 합니다.**

- [엘라스틱서치 구축 방안](../../../../DevOps_solutions/Elastic_모음/README.md#elasticsearch-id--password-생성---로컬-환경)

## overview
- elasticsearch bulk vs save

bulk API를 통해 색인을 진행합니다.
- 모든 데이터를 색인하기 때문에 , 성능 향상을 위해서 bulk API로 진행합니다.

bulk API 는 , 

    Elasticsearch의 Bulk API는 다수의 인덱스/업데이트/삭제 작업을 단일 API 호출로 실행할 수 있게 해주는 기능입니다. 

    이를 통해 데이터를 대량으로 Elasticsearch에 삽입하거나, 

    여러 작업을 한 번의 호출로 실행할 수 있습니다.

- [bulk API 관련정보](https://esbook.kimjmin.net/04-data/4.3-_bulk)

bulk API는 json파일로 색인되게 됩니다.
```bash
# bulk.json 파일 내용
{"index":{"_index":"test","_id":"1"}}
{"field":"value one"}
{"index":{"_index":"test","_id":"2"}}
{"field":"value two"}
{"delete":{"_index":"test","_id":"2"}}
{"create":{"_index":"test","_id":"3"}}
{"field":"value three"}
{"update":{"_index":"test","_id":"1"}}
{"doc":{"field":"value two"}}
```

## 참고 문서
batch 사용방안 정리
- https://velog.io/@cho876/Spring-Batch-job-%EC%83%9D%EC%84%B1

batch 최신 변동사항 정리
- https://alwayspr.tistory.com/49

batch 생성 스키마 정리
- https://zzang9ha.tistory.com/426

ElasticSearch 형태소분석 참고문서
- https://esbook.kimjmin.net/06-text-analysis/6.7-stemming/6.7.2-nori

## RDB Table 정보
전체 정보 테이블과 , 대분류 카테고리 테이블, 중분류 카테고리 테이블로 나뉩니다.

이 세가지 테이블을 Join하여 ElasticSearch에 색인할 예정입니다.
>차후 고정된 대분류와 중분류가 늘어나거나 줄어들 수 있기 떄문에 , 카테고리값만을 관리하는 테이블을 생성하고, 둘을 연결짓는 테이블로 나누었습니다.

### 1. firstInfo Table
- 실제 데이터값이 저장되는 테이블 입니다.

| 컬럼 명          |Data Type| 요약 정보        |비고|
|---------------|---|--------------|--|
| firstInfoId   |int| row별 Id 값    |PK|
| name          |varchar(20)| 컬럼이름         |-|
| age           |int| 더미 int값 넣음   |-|

### 2.  main_category
- 대분류 카테고리 정보를 관리하는 테이블 입니다.

| 컬럼 명              | Data Type   | 요약 정보       | 비고 |
|-------------------|-------------|-------------|----|
| main_category_seq | int | row별 식별자 값 (auto_increment)   | pk |
| main_category     | varchar(20) | 대분류 카테고리 명  | -  |

### 3.  sub_category
- 중분류 카테고리 정보를 관리하는 테이블 입니다.

| 컬럼 명             | Data Type   | 요약 정보       | 비고                  |
|------------------|-------------|-------------|---------------------|
| sub_category_seq | int         | row별 식별자 값 (auto_increment)   | pk |
| sub_category     | varchar(20) | 중분류 카테고리 명  | -                   |

### 4.  keyword_category
- 키워 정보를 관리하는 테이블 입니다.

| 컬럼 명        | Data Type   | 요약 정보                       | 비고                  |
|-------------|-------------|-----------------------------|---------------------|
| keyword_seq | int         | row별 식별자 값 (auto_increment) | pk |
| keyword     | varchar(20) | 키워드명                        | -                   |

### 5.  tb_ref
- firstInfo Table과 대분류 및 중분류 category 관리 테이블을 연결짓는 테이블 입니다.

| 컬럼 명           | Data Type | 요약 정보                                   | 비고 |
|----------------|-----------|-----------------------------------------|----|
| ref_seq        | int       | row별 식별자 값                              | PK |
| firstInfoId    | int       | firstInfo Table의 firstIdInfo 값을 외래키로 지정 | FK |
| main_category_seq | int       | 대분류 카테고리 식별자                            | FK |
| sub_category_seq | int | 중분류 카테고리 식별자                            | FK |
|         keyword_seq       | int | 키워드 식별자                                 | FK |



### Join Query
- 수행할 Join Query는 다음과 같습니다.

```Query
SELECT
    f.firstInfoId,
    f.name as '이름',
    f.age as '나이',
    m.main_category as '대분류',
    s.sub_category as '중분류'
FROM
    firstInfo f
        JOIN
    tb_ref r ON f.firstInfoId = r.firstInfoId
        JOIN
    tb_main_category m ON r.main_category_seq = m.main_category_seq
        JOIN
    tb_sub_category s ON r.sub_category_seq = s.sub_category_seq
ORDER BY firstInfoId ASC;
```
- 쿼리 수행 결과는 다음과 같습니다.

| firstInfoId | 이름   | 나이 | 대분류 | 중분류   |
|-------------|------|----|-----|-------|
| 0           | 감자깡  | 15 | 간식  | 과자    |
| 1           | 새우깡  | 50 | 간식  | 과자    |
| 2           | 애플워치 | 7  | 시계  | 스마트워치 |
...


## ElasticSearch 세팅
카테고리 , 이름 , age 별 검색이 가능해야 하기 때문에 doc 들에 대한 형태소 분석이 가능해야 합니다.

따라서 **nori 한글 형태소 분석기를 사용합니다.**

테스트 환경은 ElasticSearch 노드 개수가 1대이기떄문에 , 샤드 및 레플리카 개수 지정 없이 index 패턴만 설정합니다.

### nori 설치
엘라스틱서치에 nori 플러그인을 설치합니다.
>elasticsearch 홈 디렉토리에서 빈폴더안에 실행파일들 있음
>도커도 동일 : 컨테이너 내부 접근하여 설치하면 됨
```bash
// 설치
$ bin/elasticsearch-plugin install analysis-nori

// 제거
$ bin/elasticsearch-plugin remove analysis-nori
```

elasticsearch를 재 시작합니다.
```bash
$ bin/elasticsearch restart
```

nori 형태소분석기가 잘 설치되었는지 엘라스틱서치에 쿼리를 보내서 확인합니다.

일반 standard tokenizer를 사용하면 , 공백만 자를 수 있습니다.
```bash
GET _analyze
{
  "tokenizer": "standard",
  "text": [
    "동해물과 백두산이"
  ]
}

# response
{
  "tokens" : [
    {
      "token" : "동해물과",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<HANGUL>",
      "position" : 0
    },
    {
      "token" : "백두산이",
      "start_offset" : 5,
      "end_offset" : 9,
      "type" : "<HANGUL>",
      "position" : 1
    }
  ]
}
```

nori 형태소분석기를 tokenizer로 사용해서 테스트하면 , 한국어 사전 정보를 통해 어근 , 합성어등을 분리시킬 수 있습니다.

#### nori 형태소분석기 옵션들
nori_tokenizer에는 다음과 같은 옵션이 있습니다.
- ***user_dictionary :*** 사용자 사전이 저장된 파일의 경로를 입력합니다.
- ***user_dictionary_rules :*** 사용자 정의 사전을 배열로 입력합니다.
- ***decompound_mode :*** 합성어의 저장 방식을 결정합니다. 다음 3개의 값을 사용 가능합니다.
  - ***none :*** 어근을 분리하지 않고 완성된 합성어만 저장합니다.
  - ***discard (디폴트) :*** 합성어를 분리하여 각 어근만 저장합니다.
  - ***mixed :*** 어근과 합성어를 모두 저장합니다.

```bash
GET _analyze
{
  "tokenizer": "nori_tokenizer",
  "text": [
    "동해물과 백두산이"
  ]
}

# response
{
  "tokens" : [
    {
      "token" : "동해",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "물",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "과",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "백두",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "산",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "이",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "word",
      "position" : 5
    }
  ]
}
```

### index 생성
nori tokenizer를 사용하는 index를 생성합니다.
- 인덱스 명 : info_index

각 파라미터별 설명은 다음과 같습니다.
- analysis의 analyzer를 custom type으로 지정 , tokenizer nori_tokenizer 지정,
- nori_tokenizer decompound_mode를 mixed로 지정
  - ***mixed*** : 어근과 합성어를 모두 저장
  - [nori-형태소분석기-옵션들](#nori-형태소분석기-옵션들)
- ***category*** 필드를 ***nested*** 타입으로 지정
  - nested 타입으로 지정함으로써, main_category와 sub_category를 계층적 구조로 검색할 수 있게끔 구성
  - [nested 타입이란?](https://esbook.kimjmin.net/07-settings-and-mappings/7.2-mappings/7.2.5-object-nested#nested)
- name , age , 대 / 중 분류 category 검색이 가능
  - category가 과자 라면 , 과자입니다 검색했을경우에 과자 추출
  - name이 주진성 이라면 , 주진성2, 주진성이다. 주진성입니다 검색했을 경우 주진성 추출
  - 과자 주진성 모두 검색하면 , 과자 카테고리 및 주진성 이름 추출
  - 간식 이면서 과자인 문서 검색가능
  - 간식들 검색 가능
  - 과자들 검색 가능
```bash
PUT info_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "korean": {
          "type": "custom",
          "tokenizer": "nori_tokenizer"
        }
      },
      "tokenizer": {
        "nori_tokenizer": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "category": {
        "type": "nested",
        "properties": {
          "main_category": {
            "type": "text",
            "analyzer": "korean"
          },
          "sub_category": {
            "type": "text",
            "analyzer": "korean"
          }
        }
      },
      "name": {
        "type": "text",
        "analyzer": "korean"
      },
      "age":{
        "type": "integer"
      }
    }
  }
}
```

### 검색결과 확인
생성한 index에 대해 검색쿼리를 날려보면 , 요구사항에 부합하게 잘 작동하는것을 확인할 수 있습니다.
>아래쿼리 날리면 , 카테고리가 과자 또는 자연인 doc을 모두 검색해옵니다.
```bash
curl info_index/_search
{
  "query": {
    "match": {
      "category": "과자자자연"
    }
  }
}
```

>아래쿼리 날리면, 카테고리가 과자인 doc들과 ,  이름이 겔럭시워치 인 doc을 모두 검색해옵니다.
```bash
curl info_index/_search
{
    "query": {
        "bool": {
            "should": [  # should 로 쿼리 수행결과 가중치 높힘
                {
                    "match_phrase": { # match_phrase 로 쿼리가 정확히 맞는 doc만 추출
                        "name": {
                            "query": "겔럭시워치"
                        }
                    }
                },
                {
                    "match":{
                        "category": "과자"
                    }
                }
            ]
        }
    }
}
```

쿼리문법은 아래 문서를 참고
- [ElasticSearch 가이드북](https://esbook.kimjmin.net/05-search)

## dependency
아래 의존성 추가합니다.
- JPA 페이징 사용하여 select 해야되기때문에 JPA 추가
- elasticSearch에 색인해야하기 때문에 spring-boot-starter-data-elasticsearch 추가
>해당 예제에선 3.1.2 버전 사용

```gradle
// elasticSearch 에 색인 하기 위한 dependency
// https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-elasticsearch
implementation 'org.springframework.boot:spring-boot-starter-data-elasticsearch:3.1.2'

//JPA
implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
```

## 프로세스
![ElasticSearch 색인 아키텍처](./Images/batchJobArch.png)

## 코드 설명
### Entity 객체 생성
select 결과값의 Entity 객체와 , Processor 계층에서 변환되어 ElasticSearch로 색인될 Entity 객체를 생성합니다.
- [Entity Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Entity)

elasticSearch로 ```InfoDtoIndex``` 를 색인해야하기 때문에 , 어노테이션으로 elasticSearch index 정보를 생성해줍니다.
- [InfoDtoIndex Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Entity/InfoDtoIndex.java)
>각 어노테이션 별 설명은 주석 처리 해두었 습니다.

### ElasticsearchTemplate 메서드 초기화
색인되는 데이터가 바로바로 검색될 수 있게끔 , ElasticsearchTemplate 메서드를 재 정의하여 사용합니다.
- [ElasticTemplateGlobalConfig 재정의](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/ElasticTemplateGlobalConfig.java)

### Reader 생성
RDB에 저장된 각 필드별 카테고리를 elasticSearch에 색인 해야 하기에 , 카테고리 테이블 , 메인 테이블을 Join한 쿼리를 수행 하여 결과를 리턴받습니다.

**ItemReader 구현체인 JdbcPagingItemReader 를 리턴함으로써 Reader 객체를 생성합니다.**

- [Reader Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Reader)
>해당코드의 설명은 주석 처리 해두었 습니다.

### Processor 생성
Reader에서 반환받은 InfoDto 엔티티를 InfoDtoIndex 엔티티로 변환 합니다.

- [Processor Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Processor)

### Writer 생성
Processor 계층에서 변환된 InfoDtoIndex 엔티티들을 모두 elasticSearch에 색인 합니다.

색인 시 , _bulk API를 통해서 색인합니다.
- [bulk API 관련 문서](https://esbook.kimjmin.net/04-data/4.3-_bulk)
>save와 bulk 두가지중 고민
> 
>save : 단일 데이터를 색인하기 위해 설계됨
>bulk : 여러개 데이터 (대량) 를 색인하기 위해 설계됨
> 
>위 테스트 코드는 RDB의 전체 데이터를 색인하기 때문에 , bulk 사용

- [Writer Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Writer)

### Job과 Step 생성
Job , Step , 병렬처리를 위한 TaskExcutor 메서드를 생성합니다.

- [Job, Step Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/ElasticBatchGlobalConfig.java)

### JobLauncher 생성
만든 Job을 수행할 JobLauncher를 외부 모듈 (jenkins 등..) 을 사용할 수 있지만 , 해당 코드에선 코드내부에 ```@Scheduled(cron = "0 * * * * *")``` 로 크론 탭 생성하여 Job 수행 합니다.

- [JobLauncher Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/BatchJobConfig.java)

