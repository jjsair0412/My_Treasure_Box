# Batch Project 사용방안
## OverView
RDB의 전체 데이터를 Springbatch가 일정 주기로 수집하여 ElasticSearch에 색인합니다.

ElasticSearch 최종 색인 결과 기댓값은 다음과 같습니다.
```bash
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "info_index_new",
        "_id" : "qZvUsIoBagFv11qEo-v_",
        "_score" : 1.0,
        "_source" : {
          "firstInfoId" : 1,
          "name" : "1_사진",
          "age" : 30,
          "keyword" : [
            "새벽",
            "자연"
          ],
          "categories" : [
            {
              "main_category" : "사진",
              "sub_category" : "일반인"
            }
          ]
        }
      },
      {
        "_index" : "info_index_new",
        "_id" : "ppvUsIoBagFv11qEo-v_",
        "_score" : 1.0,
        "_source" : {
          "firstInfoId" : 0,
          "name" : "1_영상",
          "age" : 11,
          "keyword" : [
            "밤"
          ],
          "categories" : [
            {
              "main_category" : "영상",
              "sub_category" : "전문가"
            }
          ]
        }
      },
  ...계속...
```

## 사전 사항 : 구축대상 정보 확인
또한 구축 대상 버전은 다음과 같습니다.

대상 버전은 다음과 같습니다.
>elasticsearch 와 logstash 는 무료 버전인 7.10.0 을 사용합니다.
>
>7.10.2 버전부터 elasticsearch 가 유료로 변경되었습니다.

|               |         |    |
|---------------|---------|----|
| name          | version | 비고 |
| ElasticSearch | 7.10.0  | -  |
| logstash      | 7.10.0  | -  |


## 사전 사항 1 - elasticsearch 구축
elastic Search를 아래 문서를 따라 구축합니다.

### ***주의***
elastic search API들은 elastic search에 접근할 때 , 토큰기반 인증 또는 ID & PWD 기반 인증을 해야 합니다.

**따라서 elasticsearch를 구축할 때 , 유저를 같이 생성해줘야 합니다.**

- [엘라스틱서치 구축 방안](../../../../DevOps_solutions/Elastic_모음/README.md#elasticsearch-id--password-생성---로컬-환경)

## 사전 사항 2 - Logstash 구축
logstash를 아래 문서를 따라 구축합니다.

- [LogStash 도입기](../../../../DevOps_solutions/Elastic_모음/Logstash/Logstash_docker_install.md)

### Logstash pipeline 구축
ElasticSearch의 색인된 데이터 중 , 중복된 문서를 제거하기 위해 Logstash 구성 파일 및 pipeline을 구축합니다.

- [ElasticSearch 중복값 제거 pipeline](../../../../DevOps_solutions/Elastic_모음/Logstash/Pipeline/ElasticSearch_중복값_제거.md)

## overview
- elasticsearch bulk vs save

bulk API를 통해 색인을 진행합니다.
- 모든 데이터를 색인하기 때문에 , 성능 향상을 위해서 bulk API로 진행합니다.

bulk API 는 , 

    Elasticsearch의 Bulk API는 다수의 인덱스/업데이트/삭제 작업을 단일 API 호출로 실행할 수 있게 해주는 기능입니다. 

    이를 통해 데이터를 대량으로 Elasticsearch에 삽입하거나, 

    여러 작업을 한 번의 호출로 실행할 수 있습니다.

- [bulk API 관련정보](https://esbook.kimjmin.net/04-data/4.3-_bulk)

bulk API는 json파일로 색인되게 됩니다.
```bash
# bulk.json 파일 내용
{"index":{"_index":"test","_id":"1"}}
{"field":"value one"}
{"index":{"_index":"test","_id":"2"}}
{"field":"value two"}
{"delete":{"_index":"test","_id":"2"}}
{"create":{"_index":"test","_id":"3"}}
{"field":"value three"}
{"update":{"_index":"test","_id":"1"}}
{"doc":{"field":"value two"}}
```

## 참고 문서
batch 사용방안 정리
- https://velog.io/@cho876/Spring-Batch-job-%EC%83%9D%EC%84%B1

batch 최신 변동사항 정리
- https://alwayspr.tistory.com/49

batch 생성 스키마 정리
- https://zzang9ha.tistory.com/426

ElasticSearch 형태소분석 참고문서
- https://esbook.kimjmin.net/06-text-analysis/6.7-stemming/6.7.2-nori

## RDB 정보
전체 정보 테이블과 , 대분류 카테고리 테이블, 중분류 카테고리 테이블로 나뉩니다.

이 세가지 테이블을 Join하여 ElasticSearch에 색인할 예정입니다.
>차후 고정된 대분류와 중분류가 늘어나거나 줄어들 수 있기 떄문에 , 카테고리값만을 관리하는 테이블을 생성하고, 둘을 연결짓는 테이블로 나누었습니다.

또한 , 각 문서별로 keyword가 여러개 있을 수 있기 때문에, Reader에서 쿼리를 두번 수행합니다.
- 1. fisrtInfoId, keyword 조회
- 2. 전체정보 조회


### 1. firstInfo Table
- 실제 데이터값이 저장되는 테이블 입니다.

| 컬럼 명          |Data Type| 요약 정보        |비고|
|---------------|---|--------------|--|
| firstInfoId   |int| row별 Id 값    |PK|
| name          |varchar(20)| 컬럼이름         |-|
| age           |int| 더미 int값 넣음   |-|

### 2.  main_category
- 대분류 카테고리 정보를 관리하는 테이블 입니다.

| 컬럼 명              | Data Type   | 요약 정보       | 비고 |
|-------------------|-------------|-------------|----|
| main_category_seq | int | row별 식별자 값 (auto_increment)   | pk |
| main_category     | varchar(20) | 대분류 카테고리 명  | -  |

### 3.  sub_category
- 중분류 카테고리 정보를 관리하는 테이블 입니다.

| 컬럼 명             | Data Type   | 요약 정보       | 비고                  |
|------------------|-------------|-------------|---------------------|
| sub_category_seq | int         | row별 식별자 값 (auto_increment)   | pk |
| sub_category     | varchar(20) | 중분류 카테고리 명  | -                   |

### 4.  keyword_category
- 키워 정보를 관리하는 테이블 입니다.

| 컬럼 명        | Data Type   | 요약 정보                       | 비고                  |
|-------------|-------------|-----------------------------|---------------------|
| keyword_seq | int         | row별 식별자 값 (auto_increment) | pk |
| keyword     | varchar(20) | 키워드명                        | -                   |

### 5.  tb_ref
- firstInfo Table과 대분류 및 중분류 category 관리 테이블을 연결짓는 테이블 입니다.

| 컬럼 명           | Data Type | 요약 정보                                   | 비고 |
|----------------|-----------|-----------------------------------------|----|
| ref_seq        | int       | row별 식별자 값                              | PK |
| firstInfoId    | int       | firstInfo Table의 firstIdInfo 값을 외래키로 지정 | FK |
| main_category_seq | int       | 대분류 카테고리 식별자                            | FK |
| sub_category_seq | int | 중분류 카테고리 식별자                            | FK |
|         keyword_seq       | int | 키워드 식별자                                 | FK |



### Join Query
- 수행할 Join Query는 다음 두가지입니다.
1. 전체 정보 (firstInfoId, name, age, main_category, sub_category) 를 조인 해서 가져오는 쿼리
2. 부분 정보 (firstInfoId, keyword) 를 조인 해서 가져오는 쿼리
   
2번의 쿼리를 수행하여 firstInfoId값에 대응되는 keyword를 수집하여 , 

실제 1번 쿼리를 수행하고 entity를 생상할 때 , 같은 firstInfoId의 row는 keyword를 묶도록 작성하였습니다.
- [실제 코드](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Reader/RdbReaderImpl.java)

- 1의 수행 쿼리
```Query
select firstInfo.firstInfoId,
       name,
       age,
       main.main_category,
       sub.sub_category,
       word.keyword
from
    firstInfo
        JOIN
    tb_ref ON firstInfo.firstInfoId = tb_ref.firstInfoId
        JOIN
    tb_main_category main ON tb_ref.main_category_seq = main.main_category_seq
        JOIN
    tb_sub_category sub on tb_ref.sub_category_seq = sub.sub_category_seq
        JOIN
    tb_keyword word on tb_ref.keyword_seq = word.keyword_seq;
```

- 1의 쿼리 수행 결과는 다음과 같습니다.

| firstInfoId | name | age | main_category | sub_category | keyword |
|-------------|------|-----|---------------|--------------|---------|
| 4           | 2_사진 | 122 | 사진            | 전문가           | 맛있다     |
| 4           | 2_사진 | 122 | 사진            | 전문가           | 힐링이다    |
| 0           | 1_영상 | 11  | 영상            | 전문가          | 밤       |
...


- 2의 수행 쿼리
```Query
select
    firstInfo.firstInfoId,
    word.keyword
from
    firstInfo
        JOIN
    tb_ref ON firstInfo.firstInfoId = tb_ref.firstInfoId
        JOIN
    tb_keyword word on tb_ref.keyword_seq = word.keyword_seq;
```
- 2의 쿼리 수행 결과는 다음과 같습니다.

| firstInfoId | keyword |
|-------------|---------|
| 4           | 맛있다     |
| 4           | 힐링이다    |
| 0           | 밤       |
...


## ElasticSearch 세팅
카테고리 , 이름 , age 별 검색이 가능해야 하기 때문에 doc 들에 대한 형태소 분석이 가능해야 합니다.

따라서 **nori 한글 형태소 분석기를 사용합니다.**

테스트 환경은 ElasticSearch 노드 개수가 1대이기떄문에 , 샤드 및 레플리카 개수 지정 없이 index 패턴만 설정합니다.

### nori 설치
엘라스틱서치에 nori 플러그인을 설치합니다.
>elasticsearch 홈 디렉토리에서 빈폴더안에 실행파일들 있음
>도커도 동일 : 컨테이너 내부 접근하여 설치하면 됨
```bash
// 설치
$ bin/elasticsearch-plugin install analysis-nori

// 제거
$ bin/elasticsearch-plugin remove analysis-nori
```

elasticsearch를 재 시작합니다.
```bash
$ bin/elasticsearch restart
```

nori 형태소분석기가 잘 설치되었는지 엘라스틱서치에 쿼리를 보내서 확인합니다.

일반 standard tokenizer를 사용하면 , 공백만 자를 수 있습니다.
```bash
GET _analyze
{
  "tokenizer": "standard",
  "text": [
    "동해물과 백두산이"
  ]
}

# response
{
  "tokens" : [
    {
      "token" : "동해물과",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<HANGUL>",
      "position" : 0
    },
    {
      "token" : "백두산이",
      "start_offset" : 5,
      "end_offset" : 9,
      "type" : "<HANGUL>",
      "position" : 1
    }
  ]
}
```

nori 형태소분석기를 tokenizer로 사용해서 테스트하면 , 한국어 사전 정보를 통해 어근 , 합성어등을 분리시킬 수 있습니다.

#### nori 형태소분석기 옵션들
nori_tokenizer에는 다음과 같은 옵션이 있습니다.
- ***user_dictionary :*** 사용자 사전이 저장된 파일의 경로를 입력합니다.
- ***user_dictionary_rules :*** 사용자 정의 사전을 배열로 입력합니다.
- ***decompound_mode :*** 합성어의 저장 방식을 결정합니다. 다음 3개의 값을 사용 가능합니다.
  - ***none :*** 어근을 분리하지 않고 완성된 합성어만 저장합니다.
  - ***discard (디폴트) :*** 합성어를 분리하여 각 어근만 저장합니다.
  - ***mixed :*** 어근과 합성어를 모두 저장합니다.

```bash
GET _analyze
{
  "tokenizer": "nori_tokenizer",
  "text": [
    "동해물과 백두산이"
  ]
}

# response
{
  "tokens" : [
    {
      "token" : "동해",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "물",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "과",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "백두",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "산",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "이",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "word",
      "position" : 5
    }
  ]
}
```

### index 정의
nori tokenizer를 사용하는 index를 생성합니다.
- 인덱스 명 : info_index

#### 요구사항 정의
elasticSearch index에 대해 기대되는 요구사항은 다음과 같습니다.
- name , age , 대 / 중 분류 category 검색이 가능
  - 대 분류 : 영상 | 사진
  - 중 분류 : 일반인 | 전문가
- 키워드별 검색 가능
  - 키워드는 무한대로 늘어날 수 있음

#### index 생성
각 파라미터별 설명은 다음과 같습니다.
- analysis의 analyzer를 custom type으로 지정 , tokenizer nori_tokenizer 지정,
- nori_tokenizer decompound_mode를 mixed로 지정
  - ***mixed*** : 어근과 합성어를 모두 저장
  - [nori-형태소분석기-옵션들](#nori-형태소분석기-옵션들)
- ***category*** 필드를 ***nested*** 타입으로 지정
  - nested 타입으로 지정함으로써, main_category와 sub_category를 계층적 구조로 검색할 수 있게끔 구성
  - [nested 타입이란?](https://esbook.kimjmin.net/07-settings-and-mappings/7.2-mappings/7.2.5-object-nested#nested)

```bash
PUT info_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "korean": {
          "type": "custom",
          "tokenizer": "nori_tokenizer"
        }
      },
      "tokenizer": {
        "nori_tokenizer": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "analyzer": "korean"
      },
      "age":{
        "type": "integer"
      },
      "keyword": {
        "type": "text",
        "analyzer": "korean"
      },
      "category": {
        "type": "nested",
        "properties": {
          "main_category": {
            "type": "text",
            "analyzer": "korean"
          },
          "sub_category": {
            "type": "text",
            "analyzer": "korean"
          }
        }
      }
    }
  }
}
```

### 검색결과 확인
생성한 index에 대해 검색쿼리를 날려보면 , 요구사항에 부합하게 잘 작동하는것을 확인할 수 있습니다.
>아래쿼리 날리면 , 카테고리가 과자 또는 자연인 doc을 모두 검색해옵니다.
```bash
curl info_index/_search
{
  "query": {
    "match": {
      "category": "과자자자연"
    }
  }
}
```

>아래쿼리 날리면, 카테고리가 과자인 doc들과 ,  이름이 겔럭시워치 인 doc을 모두 검색해옵니다.
```bash
curl info_index/_search
{
    "query": {
        "bool": {
            "should": [  # should 로 쿼리 수행결과 가중치 높힘
                {
                    "match_phrase": { # match_phrase 로 쿼리가 정확히 맞는 doc만 추출
                        "name": {
                            "query": "겔럭시워치"
                        }
                    }
                },
                {
                    "match":{
                        "category": "과자"
                    }
                }
            ]
        }
    }
}
```

쿼리문법은 아래 문서를 참고
- [ElasticSearch 가이드북](https://esbook.kimjmin.net/05-search)

## dependency
### ElasticSearch 7.x.x 버전일 경우
아래 의존성 추가합니다.
- elasticSearch에 색인해야하기 때문에 data:spring-data-elasticsearch 추가
- RestClient 사용하기 때문에 spring-web 사용
>해당 예제에선 3.1.2 버전 사용

```gradle
// elasticSearch가 7.x.x 버전일땐 이 두가지를 써야됨.
// spring-web , spring-data-elasticsearch
implementation 'org.springframework.data:spring-data-elasticsearch:4.2.2'
implementation 'org.springframework:spring-web'
```

### ElasticSearch 8.x.x 버전일 경우
아래 의존성 추가합니다.
- elasticSearch에 색인해야하기 때문에 data:spring-data-elasticsearch 추가
- RestClient 사용하기 때문에 spring-web 사용
>해당 예제에선 3.1.2 버전 사용

```gradle
// elasticSearch 에 색인 하기 위한 dependency
// https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-elasticsearch
//    implementation 'org.springframework.boot:spring-boot-starter-data-elasticsearch:3.1.2'
```
## 프로세스
![ElasticSearch 색인 아키텍처](./Images/batchJobArch.png)

## 코드 설명
### Entity 객체 생성
select 결과값의 Entity 객체와 , Processor 계층에서 변환되어 ElasticSearch로 색인될 Entity 객체를 생성합니다.
- [Entity Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Entity)

elasticSearch로 ```InfoDtoIndex``` 를 색인해야하기 때문에 , 어노테이션으로 elasticSearch index 정보를 생성해줍니다.
- [InfoDtoIndex Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Entity/InfoDtoIndex.java)
>각 어노테이션 별 설명은 주석 처리 해두었 습니다.

### ElasticSearch 연결 설정
#### 버전에 따라 다릅니다.
#### ***ElasticSearch가 7.x.x 버전일 경우***
RestHighLevelClient 메서드를 재 정의하여 사용합니다.
- [RestHighLevelClient 재정의](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/ElasticConfigure/Version_7)

#### ***ElasticSearch가 8.x.x 버전일 경우***
ElasticsearchTemplate 메서드를 재 정의하여 사용합니다.
- [ElasticTemplateGlobalConfig 재정의](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/ElasticConfigure/Version_8)

### Reader 생성
RDB에 저장된 각 필드별 카테고리를 elasticSearch에 색인 해야 하기에 , 카테고리 테이블 , 메인 테이블을 Join한 쿼리를 수행 하여 결과를 리턴받습니다.

**ItemReader 구현체인 JdbcPagingItemReader 를 리턴함으로써 Reader 객체를 생성합니다.**

- [Reader Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Batch/Reader)
>해당코드의 설명은 주석 처리 해두었 습니다.

### Processor 생성
Reader에서 반환받은 InfoDto 엔티티를 InfoDtoIndex 엔티티로 변환 합니다.

- [Processor Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Batch/Processor)

### Writer 생성
#### 버전에 따라 다릅니다.
#### ***ElasticSearch가 7.x.x 버전일 경우***

Processor 계층에서 변환된 InfoDtoIndex 엔티티들을 모두 elasticSearch에 색인 합니다.

색인 시 , _bulk API를 통해서 색인합니다.
- [bulk API 관련 문서](https://esbook.kimjmin.net/04-data/4.3-_bulk)
>save와 bulk 두가지중 고민
>
>save : 단일 데이터를 색인하기 위해 설계됨
>bulk : 여러개 데이터 (대량) 를 색인하기 위해 설계됨
>
>위 테스트 코드는 RDB의 전체 데이터를 색인하기 때문에 , bulk 사용
> 
>또한 색인 시 , upsert 방식을 사용하여 기존 doc이 있다면 덮어쓰여지도록 하여 중복된 데이터가 없도록 수정합니다.

- [Writer Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Batch/Writer/Version_7_IndexWriterImpl.java)

#### ***ElasticSearch가 8.x.x 버전일 경우***

Processor 계층에서 변환된 InfoDtoIndex 엔티티들을 모두 elasticSearch에 색인 합니다.

색인 시 , _bulk API를 통해서 색인합니다.
- [bulk API 관련 문서](https://esbook.kimjmin.net/04-data/4.3-_bulk)
>save와 bulk 두가지중 고민
>
>save : 단일 데이터를 색인하기 위해 설계됨
>bulk : 여러개 데이터 (대량) 를 색인하기 위해 설계됨
>
>위 테스트 코드는 RDB의 전체 데이터를 색인하기 때문에 , bulk 사용
>
>또한 색인 시 , upsert 방식을 사용하여 기존 doc이 있다면 덮어쓰여지도록 하여 중복된 데이터가 없도록 수정합니다. 
>Version8에선 upsert관련 코드가 작성 전에 있습니다.

- [Writer Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Service/Batch/Writer/Version_8_IndexWriterImpl.java)

### Job과 Step 생성
Job , Step , 병렬처리를 위한 TaskExcutor 메서드를 생성합니다.

- [Job, Step Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/ElasticBatchGlobalConfig.java)

### JobLauncher 생성
만든 Job을 수행할 JobLauncher를 외부 모듈 (jenkins 등..) 을 사용할 수 있지만 , 해당 코드에선 코드내부에 ```@Scheduled(cron = "0 * * * * *")``` 로 크론 탭 생성하여 Job 수행 합니다.

- [JobLauncher Code](./src/main/java/com/example/indexinitbatch/elasticIndexing/Config/BatchJobConfig.java)

