# 데이터 접근 방법론 (Resampling)
**기본적으로 데이터 포인트가 비슷한 공간에 위치한다는것은 , 비슷한 특성을 가지고 있다는것을 이해하고 읽어야함.**

## Over Sampling
방법에 정답은 없고 데이터 특성에 맞게 알고리즘을 선택해야함.
- [Over Sampling 관련 문서](https://velog.io/@cardy20/Approach-to-solve-imbalanced-dataset)

### 1. SMOTE (Synthetic Minority Over-sampling)
소수 클래스 데이터 포인트들을 이용해서 새로운 데이터 를 생성하여 데이터셋을 균형화 하는 방법

생성되는 데이터 포인트는 소수 클래스 데이터 포인트 한개를 특정짓고 특정된 소수 데이터 포인트의 주변 소수 데이터 포인트를 랜덤하게 선택함. 이후 그들 사이에서 소수 클래스 데이터포인트를 생성함.

***소수 데이터 포인트에 근접하게 있다는것은 비슷한 데이터라는뜻이며, 소수 데이터 포인트를 더 생성한다는것임.***

다양한 데이터 생성이 가능하고 모델의 일반화능력을 키울 수 있지만, 데이터 중복이 발생될 수 있다는 단점을 가짐.

### 2. ADASYN (Adaptive Synthetic Sampling)
소수 클래스 데이터 포인트들의 거리를 기준으로 가중치를 매겨서 소수 클래스 데이터를 생성하는것.

가중치를 고려해서 동적으로 합성함. 

예를들어 소수클래스 데이터를 하나 특정하고, 주변의 소수 클래스 데이터들을 이어줌. 그리고 클래스간 거리에 따라 가중치를 동적으로 조절함. (거리가 멀다면 가중치를 높게, 거리가 가까우면 가중치를 낮게)
그리고 멀면 멀수록 소수 클래스 데이터를 더 생성하는 방법. 

상세한 순서로는 다음과 같음. (By_ChatGPT)
- 1. 가장 가까운 이웃 찾기: 소수 클래스 데이터 포인트를 하나 선택하고, 그 데이터 포인트로부터 가장 가까운 이웃을 찾습니다. 이 이웃은 다수 클래스와 소수 클래스 모두 포함할 수 있습니다.
- 2. 가중치 계산: 선택된 소수 클래스 데이터 포인트에 대해, 다수 클래스 이웃의 비율에 따라 가중치를 계산합니다. 즉, 다수 클래스 이웃이 많을수록 해당 소수 클래스 데이터 포인트 주변에서 더 많은 새로운 데이터 포인트를 생성할 필요가 있음을 의미합니다.
- 3. 새로운 데이터 포인트 생성: 계산된 가중치에 기반하여, 각 소수 클래스 데이터 포인트 주변에서 새로운 데이터 포인트를 생성합니다. 이때, 분류 경계선에 가까운 소수 클래스 데이터 포인트(즉, 다수 클래스 이웃이 많은 데이터 포인트) 주변에서 더 많은 데이터를 생성합니다.

***주변에 소수 클래스가 많다는것, 소수 클래스 밀집도가 높은 공간에 위치했을땐 새로운 소수 데이터 포인트를 생성할 필요성이 상대적으로 낮음.***

***반면, 다수 클래스 데이터 포인트들과 가까운 소수 클래스 데이터 포인트는 필요한 소수 데이터 포인트를 더 많이 가지고있을 수 있기에 이러한 지역에서 더 많은 소수 클래스 데이터 포인트를 생성함으로써 모델이 분류 경계를 더 잘 학습하도록 돕는 것임.***


SMOTE 보다 데이터 분포(거리)에 더욱 더 적응적이며 클래스간 거리에 따라 합성을 조절하여 불균형을 더 효율적으로 다룰 수 있지만, SMOTE에 비해 복잡하고 계산 비용을 높힐수 잇다는 단점이 있음.

## Under Sampling
### 1. Random UnderSampling
다수 클래스 데이터 포인트중 랜덤하게 선택해서 데이터를 제외하여 소수 클래스와의 균형을 맞추는 방법.

간단하고 빠르게 구현할 수 있지만, 중요 데이터가 날아가서 정보 손실이 발생할 수 있으며 선택된 데이터가 소수 클래스를 잘 대표하지 못할 수 있음.
- 해당 방법은 간단하지만 중요 데이터가 손실될수 있다는 단점이 너무 커서, 테스트할때만 사용하는것을 추천

### 2. Tomek Link UnderSampling
서로다른 클래스 간 거리가 가까운 데이터 포인트 쌍 중에서 다수 클래스의 데이터를 제거하는 방법임.

수행 과정은 다음과 같음.
- 1. 모든 다수 클래스 데이터 포인트와 모든 소수 클래스 데이터 포인트를 짝지음
- 2. 짝들의 거리를 모두 계산한다음, ***Tomek Links***를 식별함. 
    - ***Tomek Links*** : 서로 다른 클래스에 속한 데이터 포인트 쌍 중에서, 클래스 간 거리가 가장 가까운 포인트 쌍을 의미함.
- 3. 식별된 ***Tomek Links*** 를 데이터에서 제거함

**다수 클래스 데이터 포인트와 소수 클래스 데이터 포인트가 가깝다는것은, 다수 클래스 포인트가 소수 클래스 포인트와 비슷한 성향, 특징을 가지고 있다는것을 의미함.**

따라서 식별된 **Tomek Links 를 제거한다는것은 , 잘못 식별되었을 가능성, 즉 소수 클래스 데이터 포인트를 다수 클래스 포인트로 잘못 식별되었을 가능성을 줄이고, 소수 클래스를 명확히 인식할 수 있게 된다는것에 의미가 있음.**

### 3. ENN (Edited Nearest Neighbors)
클래스 간 거리가 가까운 데이터 포인트 쌍 중에서, 다수 클래스의 데이터를 제거하는 방법.

수행 과정은 다음과 같음. (By_ChatGPT)
- 1. 다수 클래스 데이터 포인트 선택: 우선, 데이터 세트에서 다수 클래스에 속하는 데이터 포인트를 하나 선택합니다.
- 2. 가장 가까운 이웃 찾기: 선택한 다수 클래스 데이터 포인트로부터 가장 가까운 k개의 데이터 포인트를 찾습니다. 이 때, '가장 가까운'이라는 기준은 일반적으로 유클리드 거리와 같은 거리 측정 방법을 사용하여 결정합니다.
- 3. 분류 확인: 찾은 k개의 가장 가까운 이웃 중에서 다수가 소수 클래스에 속한다면, 원래 선택한 다수 클래스 데이터 포인트는 소수 클래스로 잘못 분류될 가능성이 높다고 판단합니다. 즉, 이 데이터 포인트는 경계선에 가까워 분류가 어렵거나, 실제로는 소수 클래스와 비슷한 특성을 가지고 있을 수 있습니다.
- 4. 데이터 포인트 제거: 3번 단계에서 소수 클래스로 잘못 분류될 가능성이 높다고 판단된 다수 클래스의 데이터 포인트를 데이터 세트에서 제거합니다. 이 과정을 데이터 세트의 모든 다수 클래스 데이터 포인트에 대해 반복합니다.

해당 방식에서 **3번** 과정인 분류확인 과정에서, 소수 클래스가 제거될 수 도 있다 혼동할 수 있는데, 실상은 그것이 아님.

***공간적으로 비슷한 위치에 데이터 포인트가 위치한다는것은,. 데이터 포인트가 비슷한 특성을 가졋다는것과 같은 말임.***

그런데 소수 클래스가 모인곳엔 소수 클래스가 있어야하고, 다수 클래스가 모인곳엔 다수 클래스가 있어야하는데, ***왜 소수 클래스가 모인곳에 다수 클래스가 있느냐 하고 다수 클래스처럼 보이는 데이터 포인트를 제거하는것임***

***따라서 결과적으로 ENN 알고리즘을 적용하면, 모델은 정재된 다수 클래스를 가지게 되므로 , 역설적으로 소수 클래스를 더 명확히 인식할 수 있게 됨.***

## Combined Sampling
### 1. SMOTEENN
SMOTE와 ENN 두 기술을 같이 사용해서 클래스 불균형을 해결한다.

동작 방식으론 다음과 같음.
- 1. SMOTE를 이용한 오버샘플링
- 2. ENN 을 이용한 언더샘플링
- 3. 균형화된 데이터 셋 구성

두 기술을 조합함으로써 클래스 불균형을 효과적으로 해결하는것이 가능하지만, 모델 성능은 데이터셋 특성과 문제에 따라 정보손실이 발생할 수 있기에, 테스트가 중요함.