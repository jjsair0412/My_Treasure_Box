# Class Imbalance 성능 평가 및 선택
## 성능 평가의 중요성
모델의 성능을 평가하는 것은, ML 프로젝트에서 매우 중요함. 모델이 얼마나 잘 작동하고 , 예측을 얼마나 신뢰할 수 있는지를 판단함.

## 평가 Metric 선택
모델의 목적이나 특성에 따라 다양한 선능 Metric을 선택할 수 있음.

일반적으로 사용되는 Metric으로는 다음과 같은 종류가 있음.

- **1. Accuracy**
- **2. Precision**
- **3. Recall**
- **4. F1-Score**

**Accuracy가 가장 대중적인데, Class Imbalance 문제에 대해서는 해당 메트릭값을 사용하는것을 지양해야 한다.**

그러한 이유는 , 데이터셋에서 한 클래스의 사례가 다른 클래스에 비해 현저히 적을 경우(Class Imbalance), 모델이 소수 클래스의 사례를 잘못 예측해도 전체 정확도에 미치는 영향이 적을 수 있음.

***이는 모델이 대부분 다수 클래스의 사례만 정확히 예측해도 높은 정확도를 보일 수 있기 때문에, 소수 클래스에 대한 모델의 성능을 제대로 반영하지 못하는 문제가 발생합니다.***

이처럼 다양한 상황에서 다양한 Metric 지표를 선택해야 하고 , 모델의 성능을 평가하는것은 중요한 문제임.

***예를들어 Class Imbalance 문제에서 아래 3개 Metric은 모델이 다수 클래스 , 즉 Major Class로 모두 예측하는 경우에도 영향을 받지 않는 Metric 지표임.***

- **1. Precision**
- **2. Recall**
- **3. F1-Score**

## Class Imbalance 성능 평가 Metric
### 1. Precision (정밀도)
모델이 예측한 양성 클래스 중에서, 실제로 양성인 샘플의 비율

예를 들어 단순히 데이터셋이 100개인데, 양성 클래스가 80개라면 Precision은 80%

### 2. Recall
실제 양성 클래스 중에서, 모델이 양성으로 예측한 샘플의 비율

모델이 실제 양성을 놓치지 않는 데 얼마나 잘 하는지를 나타냄.

모델이 실제 양성 클래스를 틀리게 예측하지 않았는지를 표현하는 비율.

- 예를들어서 암 환자가 실제로 100명이 있는데, 모델이 100명중 10명만 암에걸렷다 예측함. -> Recall : 10%
- 예를들어서 암 환자가 실제로 100명이 있는데, 모델이 100명중 90명만 암에걸렷다 예측함. -> Recall : 90%

### 3. F1-Score
Precision과 Recall은 대체적으로 반비례함. 그러나 둘다 주요할때 해당 Metric 사용

Precision과 Recall이 비슷하게 나오고 둘다 높은것을 의미함.

### 4. ROC Curve
다양한 Threshold에서 모델이 Recall 대비 거짓 양성 비율을 나타내는 그래프.

모델 성능을 시각적으로 평가하고, Threshold 선택 중요성을 강조함.

### 5. AUC
ROC 곡선 아래 영역을 나타냄. ROC Curve가 그래프이기 때문에, 모델의 전반적 성능을 단일 숫자로 나타냄.

모델 성능을 요약할 때 유용하며, 0.5(무작위 예측) ~ 1(완벽한 에측) 사이의 값을 가짐.
- 1에 가까울수록 좋은성능 가짐.

